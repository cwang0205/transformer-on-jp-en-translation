{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21c7b4a-9c00-43c0-b7d9-19a5ec8f8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b939177-c207-4a7c-9375-eba10c90e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object to store hyperparameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 119547            # e.g., BERT-base vocab size\n",
    "        self.hidden_size = 768              # model embedding size\n",
    "        self.max_position_embeddings = 512  # maximum sequence length\n",
    "        self.hidden_dropout_prob = 0.1    # 10% probability of dropping (zeroing out) each element in the input tensor during training\n",
    "        self.intermediate_size = 3072       # FFN inner layer size (usually 4*hidden_size)\n",
    "        self.num_attention_heads = 12\n",
    "        self.num_encoder_layers = 6         # number of encoder layers\n",
    "        self.num_decoder_layers = 6         # number of decoder layers\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d35b537-85cf-439f-9cce-176eb6e1fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings module: token embeddings + positional embeddings\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) # output of this embedding lookup is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob) # define dropout rate from config param\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids shape: (batch_size, seq_length)\n",
    "        seq_length = input_ids.size(1)\n",
    "        # Create position IDs: (1, seq_length)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "        # Look up token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)           # (batch_size, seq_length, hidden_size)\n",
    "        position_embeddings = self.position_embeddings(position_ids)    # (1, seq_length, hidden_size)\n",
    "        embeddings = token_embeddings + position_embeddings  # Broadcasting over batch dimension\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        # print(\"Embeddings output shape:\", embeddings.shape) #print\n",
    "        return embeddings\n",
    "\n",
    "# attention function\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim): # embed_dim is define in hidden_size\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        return scaled_dot_product_attention(\n",
    "            self.q(query), self.k(key), self.v(value)\n",
    "        )\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, mask=None):\n",
    "        # If key or value is not provided, default to query (i.e., self-attention)\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "        # Each head returns a tensor of shape: (batch_size, seq_length, head_dim)\n",
    "        head_outputs = [h(query, key, value) for h in self.heads]\n",
    "        # Concatenate on the last dimension: shape becomes (batch_size, seq_length, embed_dim)\n",
    "        x = torch.cat(head_outputs, dim=-1)\n",
    "        # print(\"Concatenated heads shape:\", x.shape) #！！\n",
    "        x = self.output_linear(x)\n",
    "        # print(\"MultiHeadAttention output shape:\", x.shape)#！！\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size) #Expanding the dimensionality provides the model with more capacity to capture complex patterns and relationships within each token's representation\n",
    "        self.gelu = nn.GELU() #GELU is smoother than ReLU. This smooth behavior can lead to better gradient flow\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db57e689-b1b8-47f4-a771-0fc35a34e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer: attention + feed-forward, with residual connections\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-layer normalization before attention\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Residual Connection:The attention output is added back to the original x,preserve the original information while integrating new\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Pre-layer normalization before feed-forward, then\n",
    "        # residual connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder: stacking multiple encoder layers\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
    "                                     for _ in range(config.num_encoder_layers)]) # encoder_layers, defined in config\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x  # shape: (batch_size, seq_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea04661f-e3e4-481f-bf20-929370746d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Decoder Layer: has two attention sub-layers\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
    "        # Masked self-attention for decoder (prevent future tokens)\n",
    "        self.masked_attention = MultiHeadAttention(config)\n",
    "        # Encoder-decoder (cross) attention: here we assume same MultiHeadAttention; \n",
    "        # In practice, keys and values come from encoder outputs.\n",
    "        self.enc_dec_attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        \n",
    "    def forward(self, x, memory, tgt_mask=None):\n",
    "        # Masked self-attention sub-layer\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "         # residual connection\n",
    "        x = x + self.masked_attention(hidden_state, mask=tgt_mask)\n",
    "        # Encoder-decoder (cross) attention: query from decoder; key & value from encoder memory\n",
    "        hidden_state2 = self.layer_norm_2(x)\n",
    "         # residual connection\n",
    "        #  pass memory here\n",
    "        x = x + self.enc_dec_attention(hidden_state2, key=memory, value=memory)\n",
    "        # Feed-forward sub-layer & residual connection\n",
    "        x = x + self.feed_forward(self.layer_norm_3(x))\n",
    "        return x\n",
    "\n",
    "# Transformer Decoder: stacking multiple decoder layers\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)  # for target tokens\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(config)\n",
    "                                     for _ in range(config.num_decoder_layers)])\n",
    "    def forward(self, target_ids, memory, tgt_mask=None):\n",
    "        x = self.embeddings(target_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask=tgt_mask)\n",
    "        return x  # shape: (batch_size, target_seq_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf306ac-0962-4aed-850f-3bbd38a7499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "def generate_tgt_mask(seq_length, device=None):\n",
    "    \"\"\"\n",
    "    Generates a target mask for a sequence of given length.\n",
    "    \n",
    "    The mask is a lower triangular matrix of shape (1, seq_length, seq_length),\n",
    "    where positions with 1 indicate allowed attention and 0 indicate masked positions.\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): The length of the target sequence.\n",
    "        device (torch.device or None): The device on which to create the mask.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A mask tensor of shape (1, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    # Create a lower triangular matrix filled with 1s (allowed positions)\n",
    "    mask = torch.tril(torch.ones(seq_length, seq_length, device=device))\n",
    "    # Unsqueeze to add a batch dimension for broadcasting: (1, seq_length, seq_length)\n",
    "    mask = mask.unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8665b258-e638-422e-929a-48f236168fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Transformer Model: connects encoder and decoder\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.output_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        \n",
    "    def forward(self, src_ids, tgt_ids, tgt_mask=None):\n",
    "        memory = self.encoder(src_ids)\n",
    "        decoder_output = self.decoder(tgt_ids, memory, tgt_mask=tgt_mask)\n",
    "        logits = self.output_linear(decoder_output)\n",
    "        return logits  # shape: (batch_size, tgt_seq_length, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd2343-92f5-4092-b019-86af6ec6c4e0",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6931dcc2-5bd8-4e68-8851-da7e2ca1a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data=json.load(f)\n",
    "# print(\"Number of examples:\", len(data))\n",
    "# print(json.dumps(data[0], indent=2))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.shape\n",
    "\n",
    "df_exploded = df.explode(\"conversation\")\n",
    "# Reset index for convenience\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "# Normalize the \"conversation\" column into separate columns\n",
    "conversation_df = pd.json_normalize(df_exploded[\"conversation\"])\n",
    "# Merge the normalized data back into the exploded df\n",
    "df_exploded = df_exploded.drop(\"conversation\", axis=1).join(conversation_df)\n",
    "# only need the ja_sentance and en_sentence columns\n",
    "train_data=df_exploded[[\"ja_sentence\",\"en_sentence\"]]\n",
    "\n",
    "train_data=train_data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882c11a-959f-4a58-840e-c26b046e83c1",
   "metadata": {},
   "source": [
    "data prep for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f4666a-cda2-4266-aa3f-0e0ea924ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean - removing unwanted characters, normalizing punctuation, etc\n",
    "\n",
    "# tokenizing\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") # this impact token size\n",
    "\n",
    "# Tokenize sentences with special tokens (like [CLS], [SEP] or <sos>, <eos> depending on the model)\n",
    "train_data[\"src_ids\"] = train_data[\"ja_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n",
    "train_data[\"tgt_ids\"] = train_data[\"en_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n",
    "\n",
    "# padding them to a fixed length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# creates two Python lists—one for the source sentences and one for the target sentences. \n",
    "# Each element in these lists is a PyTorch tensor that contains the token IDs for one sentence.\n",
    "src_id_tensors = [torch.tensor(ids) for ids in train_data[\"src_ids\"]]\n",
    "tgt_id_tensors = [torch.tensor(ids) for ids in train_data[\"tgt_ids\"]]\n",
    "# Use the tokenizer's pad token ID for padding\n",
    "src_padded = pad_sequence(src_id_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "tgt_padded = pad_sequence(tgt_id_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "# batch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(src_padded, tgt_padded)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a907d9f-e7b8-435f-b031-b4fe24efebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source token ID: 119518\n",
      "Max target token ID: 110603\n",
      "Tokenizer vocab size: 119547\n"
     ]
    }
   ],
   "source": [
    "max_src = max([max(ids) for ids in train_data[\"src_ids\"] if ids])\n",
    "max_tgt = max([max(ids) for ids in train_data[\"tgt_ids\"] if ids])\n",
    "print(\"Max source token ID:\", max_src)\n",
    "print(\"Max target token ID:\", max_tgt)\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd42e8-d73f-498a-938d-3d6ebbbc19f2",
   "metadata": {},
   "source": [
    "\n",
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90be7208-fef8-4df3-8ca7-66ff8f407022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 11.975255012512207\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 10.264413833618164\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 10.630443572998047\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 10.061494827270508\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 9.788758277893066\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 9.845888137817383\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 9.581720352172852\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 8.95136833190918\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 8.535983085632324\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 8.344019889831543\n",
      "Epoch 1/5, Loss: 9.7979\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 7.696781635284424\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.746273040771484\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.723031044006348\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.489844799041748\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.777124404907227\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.490725994110107\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.3766632080078125\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.551151752471924\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 6.074901103973389\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.896346092224121\n",
      "Epoch 2/5, Loss: 6.5823\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.610948085784912\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.235722064971924\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.9344162940979\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.134012222290039\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.418820381164551\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.173352241516113\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.037717342376709\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.020462512969971\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 5.036481857299805\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.9237189292907715\n",
      "Epoch 3/5, Loss: 5.1526\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.622871398925781\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.584589004516602\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.642211437225342\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.489015579223633\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.110764980316162\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.339810371398926\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.32985258102417\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.125582695007324\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.4673752784729\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.9045510292053223\n",
      "Epoch 4/5, Loss: 4.3617\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.865767002105713\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 4.063342094421387\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.9929463863372803\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.8396213054656982\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.9494593143463135\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.791499614715576\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.686911106109619\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.819460868835449\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.527567148208618\n",
      "decoder_input: torch.Size([50, 40])\n",
      "target_output: torch.Size([50, 40])\n",
      "Logits shape: torch.Size([50, 40, 119547])\n",
      "Loss: 3.989473342895508\n",
      "Epoch 5/5, Loss: 3.8526\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model with your configuration and move it to the device.\n",
    "model = TransformerModel(config).to(device)\n",
    "\n",
    "# Define the loss function.\n",
    "# CrossEntropyLoss is used for classification tasks, and we ignore the pad token.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Define an optimizer, here we use Adam.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "epoch_result=[]\n",
    "# Iterate over epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Iterate over batches from the DataLoader.\n",
    "    for batch in dataloader:\n",
    "        #   src_ids: Padded token IDs for source sentences (shape: [batch_size, src_seq_len])\n",
    "        #   tgt_ids: Padded token IDs for target sentences (shape: [batch_size, tgt_seq_len])\n",
    "        src_ids, tgt_ids = batch\n",
    "        src_ids = src_ids.to(device)\n",
    "        # print(\"src:\", src_ids.shape)\n",
    "        tgt_ids = tgt_ids.to(device)\n",
    "        # print(\"tgt:\", tgt_ids.shape)\n",
    "           \n",
    "        # shift the target sequence: ensures that at every time step the model’s input is the sequence of previous tokens, and it’s trained to predict the next token.\n",
    "        #   - decoder_input: All tokens except the last one.\n",
    "        #   - target_output: All tokens except the first one.\n",
    "        decoder_input = tgt_ids[:, :-1]   # shape: (batch_size, tgt_seq_len - 1),shifting it removes one token from each end:\n",
    "        target_output = tgt_ids[:, 1:]      # shape: (batch_size, tgt_seq_len - 1)\n",
    "        print(\"decoder_input:\", decoder_input.shape)\n",
    "        print(\"target_output:\", target_output.shape)\n",
    "        \n",
    "        # Generate a target mask to enforce causality in the decoder.\n",
    "        # This mask prevents each token from attending to future tokens.\n",
    "        seq_length = decoder_input.size(1)\n",
    "        tgt_mask = generate_tgt_mask(seq_length, device=decoder_input.device)\n",
    "        \n",
    "        # Forward pass: pass source and decoder inputs through the model.\n",
    "        # The model expects (src_ids, decoder_input, tgt_mask).\n",
    "        logits = model(src_ids, decoder_input, tgt_mask=tgt_mask)\n",
    "        # logits shape: (batch_size, tgt_seq_len - 1, vocab_size)\n",
    "        print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "        # Reshape logits and target_output for loss computation.\n",
    "        # We flatten the batch and sequence dimensions.\n",
    "        logits = logits.reshape(-1, config.vocab_size)\n",
    "        target_output = target_output.reshape(-1)\n",
    "        \n",
    "        # Compute the loss.\n",
    "        loss = criterion(logits, target_output)\n",
    "        print(\"Loss:\", loss.item())\n",
    "        # Backpropagation and parameter update.\n",
    "        optimizer.zero_grad()   # Clear previous gradients\n",
    "        loss.backward()         # Compute new gradients\n",
    "        optimizer.step()        # Update the model's parameters\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "    epoch_result.append([epoch,average_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c892ccc-83ae-40ef-85a9-4457e39f734e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 9.797934532165527],\n",
       " [1, 6.582284307479858],\n",
       " [2, 5.15256519317627],\n",
       " [3, 4.361662435531616],\n",
       " [4, 3.8526048183441164]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757d0d5-a747-4adf-aa72-18ddd5fc3ddc",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ce62587-de05-4c51-a24a-9bd70250ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data \n",
    "with open(\"test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data=json.load(f)\n",
    "# print(\"Number of examples:\", len(data))\n",
    "# print(json.dumps(data[0], indent=2))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.shape\n",
    "\n",
    "df_exploded = df.explode(\"conversation\")\n",
    "# Reset index for convenience\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "# Normalize the \"conversation\" column into separate columns\n",
    "conversation_df = pd.json_normalize(df_exploded[\"conversation\"])\n",
    "# Merge the normalized data back into the exploded df\n",
    "df_exploded = df_exploded.drop(\"conversation\", axis=1).join(conversation_df)\n",
    "# only need the ja_sentance and en_sentence columns\n",
    "test_data=df_exploded[[\"ja_sentence\",\"en_sentence\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8d9a4db-df7a-44ef-b519-b0a7c7e377c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenl\\AppData\\Local\\Temp\\ipykernel_21364\\940788166.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[\"src_ids\"] = test_data[\"ja_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n",
      "C:\\Users\\chenl\\AppData\\Local\\Temp\\ipykernel_21364\\940788166.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[\"tgt_ids\"] = test_data[\"en_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n"
     ]
    }
   ],
   "source": [
    "# prep tensor for testing\n",
    "# Tokenize the test sentences (using the same tokenizer as before)\n",
    "test_data[\"src_ids\"] = test_data[\"ja_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n",
    "test_data[\"tgt_ids\"] = test_data[\"en_sentence\"].apply(lambda s: tokenizer.encode(s, add_special_tokens=True))\n",
    "\n",
    "# Convert the token lists to tensors\n",
    "src_id_tensors_test = [torch.tensor(ids) for ids in test_data[\"src_ids\"]]\n",
    "tgt_id_tensors_test = [torch.tensor(ids) for ids in test_data[\"tgt_ids\"]]\n",
    "\n",
    "# Pad the sequences\n",
    "src_padded_test = pad_sequence(src_id_tensors_test, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "tgt_padded_test = pad_sequence(tgt_id_tensors_test, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "# Create a dataset and DataLoader for test data\n",
    "test_dataset = TensorDataset(src_padded_test, tgt_padded_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc7a278a-0c99-4381-b7a5-09eef08c5614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2120, 74])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_padded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcff4655-0e13-4493-b56b-ca7179d63c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.441004620041958\n",
      "Test Accuracy: 0.2197485128058989\n"
     ]
    }
   ],
   "source": [
    "# evluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "total_correct = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        src_ids, tgt_ids = batch\n",
    "        src_ids = src_ids.to(device)\n",
    "        tgt_ids = tgt_ids.to(device)\n",
    "        \n",
    "        # Create decoder input and target output by shifting tgt_ids\n",
    "        decoder_input = tgt_ids[:, :-1]  # shape: (batch_size, tgt_seq_len - 1)\n",
    "        target_output = tgt_ids[:, 1:]     # shape: (batch_size, tgt_seq_len - 1)\n",
    "        \n",
    "        # Generate the target mask for the decoder\n",
    "        seq_length = decoder_input.size(1)\n",
    "        tgt_mask = generate_tgt_mask(seq_length, device=decoder_input.device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        logits = model(src_ids, decoder_input, tgt_mask=tgt_mask)\n",
    "        # logits shape: (batch_size, tgt_seq_len - 1, vocab_size)\n",
    "        \n",
    "        # Compute loss: flatten logits and target output for CrossEntropyLoss\n",
    "        loss = criterion(logits.reshape(-1, config.vocab_size), target_output.reshape(-1))\n",
    "        total_loss += loss.item() #accumulating the numeric loss value from each batch\n",
    "        \n",
    "        # Compute token-level accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)  # shape: (batch_size, tgt_seq_len - 1)\n",
    "        # Create a mask to ignore pad tokens in the target, creates a boolean tensor label True where the target output is not the pad token,\n",
    "        non_pad = target_output != tokenizer.pad_token_id \n",
    "        correct = (predictions == target_output) & non_pad\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += non_pad.sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_dataloader)\n",
    "accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "print(\"Test Loss:\", avg_loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfae68e-431f-4df8-8431-a52371b0e311",
   "metadata": {},
   "source": [
    "# option\n",
    "to save and load the model back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e351d2a-3808-471a-87f0-c05e566f4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dict to a file\n",
    "# torch.save(model.state_dict(), \"transformer_model.pth\")\n",
    "\n",
    "# Create a new instance of the model with the same configuration\n",
    "# model_loaded = TransformerModel(config)\n",
    "# model_loaded.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
    "# model_loaded.to(device)  # Move the model to the appropriate device (GPU or CPU)\n",
    "# model_loaded.eval()      # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ee8bc-eed0-4070-840f-f753b08b36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38ee93-fc62-4322-b898-f27a4b6942b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae7c66-624b-46a2-8dac-1ad80770dfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaff762-a837-45e6-8401-89fd3ffbd684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476110ca-c8d0-4f71-bc31-6d03db11e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d47078-10de-4f01-b97a-e1c953cbee06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
